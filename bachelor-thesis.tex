\documentclass[times,specification,annotation]{itmo-student-thesis}

%% Опции пакета:
%% - specification - если есть, генерируется задание, иначе не генерируется
%% - annotation - если есть, генерируется аннотация, иначе не генерируется
%% - times - делает все шрифтом Times New Roman, собирается с помощью xelatex
%% - pscyr - делает все шрифтом Times New Roman, требует пакета pscyr.

%% Делает запятую в формулах более интеллектуальной, например:
%% $1,5x$ будет читаться как полтора икса, а не один запятая пять иксов.
%% Однако если написать $1, 5x$, то все будет как прежде.
\usepackage{icomma}

%% Один из пакетов, позволяющий делать таблицы на всю ширину текста.
\usepackage{tabularx}

\usepackage{tikz}

\usetikzlibrary{arrows.meta, fit}

\usepackage{mathtools}

\usepackage{makecell}

%% Указываем файл с библиографией.
\addbibresource{bachelor-thesis.bib}

\begin{document}

\studygroup{M3439}
\title{Обучение метрики похожести сообществ с помощью выделения
векторного представления}
\author{Зернов Глеб Сергеевич}{Зернов Г.С.}
\supervisor{Сметанников Иван Борисович}{Сметанников И.Б.}{к.техн.н.}{ассистент, факультет информационных технологий и программирования, Университет ИТМО}
\publishyear{2019}
%% Дата выдачи задания. Можно не указывать, тогда надо будет заполнить от руки.
\startdate{18}{декабря}{2018}
%% Срок сдачи студентом работы. Можно не указывать, тогда надо будет заполнить от руки.
\finishdate{6}{мая}{2019}
%% Дата защиты. Можно не указывать, тогда надо будет заполнить от руки.
\defencedate{}{}{2019}

\addconsultant{Попов А.Л.}{магистр}

\secretary{Павлова О.Н.}

%% Задание
%%% Техническое задание и исходные данные к работе
\technicalspec{Разработать модель, которая позволит представить сообщества социальной сети в
виде векторов. Модель необходимо обучить и протестировать на анонимных неразмеченных
сессионных данных из социальной сети «Вконтакте». Необходимо проанализировать
результаты и сравнить предлагаемое решение с альтернативными методами.}

%%% Содержание выпускной квалификационной работы (перечень подлежащих разработке вопросов)
\plannedcontents{
\begin{enumerate}
\item[1.] Описание предметной области. Обзор существующих алгоритмов.
\item[2.]  Описание алгоритма векторного представления сообществ
\item[3.]  Анализ результатов, сравнение с существующими решениями.
\end{enumerate}}

%%% Исходные материалы и пособия 
\plannedsources{}

%%% Цель исследования
\researchaim{Реализация алгоритма представления сообществ социальной сети в виде векторов.}

%%% Задачи, решаемые в ВКР
\researchtargets{\begin{enumerate}
    \item Обзор существующих подходов.
    \item Разработка алгоритма
выделения векторного представления сообществ по сессионным анонимным неразмеченным
данным. 
    \item Анализ полученных результатов.
\end{enumerate}}

%%% Использование современных пакетов компьютерных программ и технологий
\addadvancedsoftware{Язык программирования \texttt{Python}}{
%%% \ref{sec:tables}, Приложения~\ref{sec:app:1}, \ref{sec:app:2}
2.3 – 3.3}
\addadvancedsoftware{Программное средство \texttt{Jupyter Notebook}}{2.3 – 3.3}
\addadvancedsoftware{Библиотека \texttt{Pytorch}}{2.3 – 2.6}
\addadvancedsoftware{Библиотека \texttt{Sklearn}}{3.1 – 3.3}
\addadvancedsoftware{Библиотека \texttt{Matplotlib}}{3.1}

%%% Краткая характеристика полученных результатов 
\researchsummary{Исследованы существующие решения данной задачи. Реализован алгоритм, позволяющий
решить поставленную задачу. Проведено сравнение предложенного решения с существующими
путем решения дополнительных задач классификации с использованием результатов работы
моделей в качестве входных данных.}

%%% Гранты, полученные при выполнении работы 
\researchfunding{При выполнении работы грантов получено не было.}

%%% Наличие публикаций и выступлений на конференциях по теме выпускной работы
\researchpublications{Нет}

%% Эта команда генерирует титульный лист и аннотацию.
\maketitle{Бакалавр}

%% Оглавление
\tableofcontents

%% Макрос для введения. Совместим со старым стилевиком.
\startprefacepage

Обычно объекты из реального мира представимы в виде набора некоторого
количества признаков, численные значения которых, в свою очередь, можно
представить в виде вектора. Таким образом, мы можем получить представление о
положении объектов относительно друг друга в полученном пространстве,
сохраняя при этом знания о конкретных признаках каждого объекта. Такие
векторы можно использовать в ряде алгоритмов машинного обучения, исходя из
простого правила: чем ближе два вектора друг к другу, тем более похожи между
собой два объекта.

Однако, существует возможность столкнуться с рядом проблем при попытке
представить объекты в виде векторов. К примеру, полученные векторы могут не
коррелировать с положением дел в реальном мире. В таком случае, выбранные
нами векторы не подходят для алгоритмов машинного обучения, поскольку
алгоритм не будет способен сопоставить положение векторов в простратсве с картиной
реального мира. Кроме того, какие-то сущности имеют либо слишком большое
число признаков, и понять, какие признаки стоит добавить в конечный вектор, а
какие отбросить, бывает крайне трудной задачей.

Решением этих проблем выступает синтетический вектор (эмбеддинг), элементы
которого сами по себе не имеют отношение к какому-то конкретному признаку, но
в совокупности данные векторы подходят для описания объекта.

%% Начало содержательной части.
\chapter{ОПИСАНИЕ ПРЕДМЕНТОЙ ОБЛАСТИ}

\startrelatedwork

\section{Основные определения}

TODO: больше определений и вводных понятий

Векторное представление или эмбеддинг (англ. Embedding) --- отображение
множества объектов на пространство векторов с сохранением некоторой
структуры за объектами.

\section{Постановка задачи}\label{sec:intro}

«Вконтакте» --- одна из крупнейших социальных сетей СНГ и самая
популярная соц. сеть в России. «Вконтакте» на ряду с многими другими
социальными сетями имеет внутри себя объединения пользователей по интересам,
называемое сообществами. Пользователи имеют возможность подписаться на
интересуемое их сообщество для более удобного взаимодействия с информацией
сообщества, а также отписаться от него. Сообщества могут добавлять на свою
страницу записи (текст, картинки, видео и т. д.), которые пользователи могут
помечать как понравившиеся («лайки»), комментировать и прочее.

Существенной сложностью работы с сообществами является число сообществ (миллионы), каждое из которых может состоять из миллионов пользователей, и содержать миллионы записей.  

Таким образом, полная информация о сообществе --- это огромный массив
данных, предоставляя который полностью некоторой модели машинного обучения,
мы не только существенно увеличиваем размер необходимого объема памяти для хранения этой информации, но и замедляем работу самого алгоритма, что делает потенциальный алгоритм бесполезным на практике.

Выходом из этой ситуации является некоторое более емкое представление сообщества без потери интересующей нас информации.

Главная задача, решаемая в моей работе, --- разработать алгоритм выделения универсального векторного представление сообществ небольшой размерности. 

Полученные векторы имеют практическую ценность. Например, они могут быть использованы в
качестве входных данных другой модели и проанализированы с целью
получить представление о сходстве пар сообществ между собой. Такие векторы содержат большое количество информации в сжатом виде, в отличие, от, например, классического one-hot (подробнее в \ref{sec:nlp-intro}) векторного представления объектов.

Формализуем поставленную задачу: для заданного множества $C$ сообществ и сессий двух типов $S$, $U$ , где сессия $s_n$ --- список последовательных $K$ подписок на сообщества пользователя $n$ за определенный промежуток времени $s_n = (c_{1}, c_{2}, \dots, c_{K})$, а $u_n$ --- список последовательных $M$ отписок от сообществ пользователя $n$: $u_n = (c_{1},  c_{2}, \dots, c_{M})$, $s_n \cap u_n = \varnothing$, $|S| = |U|$ .  
Задачей является найти представление размерности $d$ $v_{c} \in \mathbb{R}^d$ для каждого сообщества $c \in C$ так, что похожие сообщества находятся близко друг к другу в конечном векторном пространстве. 

\subsection{Формат входных данных}

Компанией были предоставлены анонимные сессионные действия пользователей за небольшой промежуток времени. Всего было предоставлено 3 набора данных.

Первый набор данных состоит из списка сессий, каждая сессия --- это список действий некоторого пользователя, элементами списка является идентификатор сообщества (id) и маркер действия (подписка/отписка). Данные были собраны по действиям пользователей за 2-3 месяца.

Так же были предоставленные сессионные данные по отметкам <<мне нравится>> пользователя (<<лайки>>) в аналогичном формате: список сессий, каждая сессия является списком id сообществ, запись которого понравились пользователю. Стоит отметить, что в отличие от данных по подпискам, в сессии по лайкам сообщество может встречаться несколько раз. Данные были собраны по действиям пользователей за 2-3 дня. Таким образом, средняя длина сессий второго набора данных примерно равна средней длины сессий из первого набора данных. 

Третий набор данных является комбинацией из первых двух, каждая сессия имеет тип того, какая информация в ней содержится: действия пользователей по подпискам или лайкам. 

После фильтрации данных общее число сессий было сокращено до 500 000, число уникальных групп до 11 500 для каждого набора данных. Средняя длина сессий всех наборов данных примерно равна 4.5

\section{Обзор существующих решений}

Представление сущностей в виде эмбеддингов является довольно распространенной задачей. Часто такие векторы являются продуктом работы некоторого алгоритма, решающего другую задачу. 

\subsection{Алгоритм факторизации матриц в рекомендательных системах.}\label{sec:als}

Персонализация пользовательского контента является одной из основополагающих частей систем, предоставляющих такой контент конечным пользователям. Лидеры интернет-торговли или интернет-услуг особенно заинтересованы в таких алгоритмах, поскольку позволяет увеличить пользовательский отклик.
В общем случае, рекомендательные системы берут за основу две стратегии: фильтрация контента (content filtering) и коллаборативная фильтрация (collaborative filtering)
  
Главная идея за фильтрацией контента состоит в том, чтобы создать профиль для каждого пользователя или продукта с целью охарактеризовать сущность. Например, в качестве профиля фильма могут выступать жанр, актеры, кассовые сборы и т.д.

Альтернативный вариант, коллаборативная фильтрация, опирается на действия пользователей, сделанных в прошлом. Такие алгоритмы анализируют статистические связи между пользователями, продуктами, их взаимодействиями и дают рекомендации, основываясь на исторических данных.  Большим плюсом таких систем является независимость от домена, в котором они применяются.

Одним из самых распространенных на данный момент времени алгоритмов в сфере рекомендательных систем является алгоритм факторизации матриц \cite{koren2009}, относящийся к группе алгоритмов коллаборативной фильтрации.

Модель матричной факторизации переносит пользователей и продукты в пространство признаков размерности $n$. Таким образом, продукт $p$ представим в виде вектора $a_{p} \in \mathbb{R}^n$ , а пользователь $u$ в виде вектора $b_{u} \in \mathbb{R}^n$. Для продукта каждый из элементов вектора отвечает за то, насколько каждый признак характерен для  этого продукта. В то время для пользователя элементы вектора показывают, насколько пользователю интерес признак.

Это позволяет задать оценку пользователя $u$ продукта $p$ в виде формулы

$r_{ui} = a_{i}^{T}b_{u}$

Таким образом, зная матрицу пользовательских оценок $R$ можно разложить ее на произведение матриц $A$ и $B$, отвечающих за векторное произведение продуктов и пользователей соответственно.

$R = A^TB$

В рамках нашей задачи составим матрицу оценок следующим образом: для подписки пользователя $i$ на сообщество $j$ поставим единицу на эту позицию $R_{ij} = 1$, аналогично для отписки $R_{ij} = - 1$. В случае отсутствия действий пользователя для сообщества оценка будет равна нулю.


\subsection{Обзор алгоритма LDA}\label{sec:lda}

Latent Dirichlet allocation (LDA) \cite{lda2003} --- вероятностная модель над корпусом документов. Базовая идея заключается в том, что документу представимы в виде набора тем, где каждая тема характеризуется распределением по словам. Данная модель позволяет выделять несколько тем для документа и рассчитывать вероятности отношения каждой из тем к документу.
В качестве продукта работы алгоритма, LDA позволяет строить вероятностные распределения тем по документу, слов по темам и темам по словам.

Например, для такой модели будет верно, что слово <<кот>> имеет большую вероятность того, что оно относится к теме, интерпретируемую наблюдателем как <<животные>> (имеет большие вероятности для слов, обозначающих животных), чем к другим темам. 

Стоит отметить, что несмотря на тот факт, что модель LDA была впервые применена в области NLP и используется в основном в контексте документов естественного языка, это не ограничивает использование алгоритма в других доменах. Так, в оригинальной статье \cite{lda2003} приводится пример использования для коллаборативного фильтрования. Кроме того, модель может быть использована для поиска групп в графах \cite{Henderson2009}, для обнаружения классов объектов на изображениях, представленных в трехмерном виде (обучение без учителя) \cite{Endres2009} и прочее.

В рамках данной работы алгоритм LDA применим следующим образом: представим сессии в виде документов, где словами являются подписки на сообщества. Применив алгоритм на корпусе сессий, получим список тем, а так же вероятность принадлежности сообщества к теме для всех сообществ. Так мы получим вектор вероятностей для каждого сообщества, которым можно охарактеризовать любое сообщество из всего словаря. Можно интерпретировать такое представление сообществ как то, что похожие сообщества будут иметь похожие вероятности тем. 
 
Заметим, однако, что модель не будет учитывать сигналы отписок от сообщества, что является недостатком применения этого алгоритма. 

\section{Модели естественного языка}\label{sec:nlp-intro}

Для множества алгоритмов обработки естественного языка (Natural Language Processing, NLP) некие специализированные цели алгоритма могут быть обобщены на задачу нахождения значений вероятностей для последовательностей слов.
Таким образом, развитие моделей естественного языка шло путем выявления статистических свойств слов и нахождения зависимостей между ними.

Традиционно такие подходы представляли каждое слово в виде one-hot вектора, где размерность вектора каждого слова равна длине словаря, а значения элементов равны 1, если номер элемента совпадает с позицией слова в словаре, и 0 иначе.  

Существенным недостатком такого подхода является наложение ограничений на практическое применение алгоритмов, в связи с большой размерностью объектов и разряженности данных, приводящих к ощутимой потери производительности.

Исследователями были предложены модели на основе нейронных сетей\cite{turian2010}, которые позволяют решить эти проблемы путем представления слов в виде векторов гораздо меньшей размерности. Такие модели основываются на гипотезе о том, что близкие друг к другу слова в предложениях статистически более зависимы.

Исторически, неэффективность обучения моделей нейронных сетей была главным препятствием на пути к применению таких алгоритмов на практике, поскольку словарь может достигать миллионов слов. Однако предложенные в \cite{mikolov2013efficient} и \cite{mikolov2013distributed} модели оказались достаточно хорошо масштабируемыми и могут быть эффективно использованы на практике. Алгоритмы оказались крайне результативными и эксперименты показали, что модели способы выявлять синтаксические и семантические связи между словами в больших корпусах слов. 

Концептуальность такого представления объектов вышла за рамки задач естественного языка и была применена в областях, с ним не связанных. Так были предложены алгоритмы векторного представления продуктов \cite{grbovic2015commerce}, рекомендательных систем \cite{ozsoy2016word} и прочие.

%\subsection{Обзор алгоритма word2vec}\label{sec:word2vec}

%Word2vec\cite{mikolov2013efficient} --- нейронная сеть с одним скрытым слоем, позволяющая
%представлять слова в виде векторов в качестве дополнительного продукта после
%обучения самой модели. Основная гипотеза, на которой строится модель, --- слова,
%встречающиеся в одном и том же (или похожем) контексте, будут похожи между
%собой (либо одинаковыми по смыслу).

%Word2vec набрал большую популярность в научном сообществе. Было показано, что данный алгоритм представляет интерес не только в рамках nature language processing, но и в других, не связанных с естественным языком, категориях. Представленный алгоритм позволяет выявлять синтаксические и семантические связи слов, что представляет определенный интерес в рамках других задач. Так, концепт 

%Так в \cite{grbovic2015commerce} был описан алгоритм, позволяющий

%Далее в главе \ref{sec:algo} будет рассмотрена модификация алгоритма word2vec для решения задачи в рамках сообществ социальной сети.

\finishrelatedwork

\chapter{ОПИСАНИЕ АЛГОРИТМА ВЫДЕЛЕНИЯ ВЕКТОРНОГО ПРЕДСТАВЛЕНИЯ СООБЩЕСТВ}

В данной главе мы рассмотрим алгоритм, позволяющий выделять векторы
сущностей, информация о которых представлена в виде сессии взаимодействия
пользователей с этими сущностями (положительные и отрицательные отклики).
Также будет рассмотрена реализация алгоритма на примере сессионных данных
пользователей «Вконтакте». 

Будут рассмотрены вариации алгоритма для данных пользователей по подпискам, затем по данным отметок <<мне нравится>> (или <<лайкам>> --- likes), и в конце по комбинированным данным.

% Сначала будут рассмотрены методы применения алгоритмов описанных в \ref{sec:als} и \ref{sec:lda} в рамках поставленной задачи.  Затем будет рассмотрена возможность модификации алгоритма \ref{sec:word2vec} для решения поставленной задачи. В конце будет описан конечный вариант алгоритма, базирующийся на \ref{sec:word2vec}.

\section{Описание алгоритма}\label{sec:algo}

\begin{figure}[!h]
\caption{skip-gram модель}\label{fig-sg}
\centering
\begin{tikzpicture}
  \node[draw, fill=yellow] (t) at (0, 0) {$c_{i}$};
  \node[draw, fill=gray!40] (tg) at (0, 1.5) {$c_{i}$};
  \node[draw, fill=green!65] (c_i1) at (1.5, 1.5) {$c_{i + 1}$};
  \node[draw, fill=green!65] (c_iw) at (3.5, 1.5) {$c_{i + w}$};
  \node[draw, fill=green!65] (c_i-1) at (-1.5, 1.5) {$c_{i - 1}$};
  \node[draw, fill=green!65] (c_i-w) at (-3.5, 1.5) {$c_{i - w}$};
 
  \node at (-2.5, 1.5) {\dots};
  \node at (2.5, 1.5) {\dots};
  \node at (-5.5, 1.5) {Сессия $s_k$};
  \node at (0, -1) {Целевое сообщество $c_i$};
  \node at (0, 2.5) {Событие в сессии $s_k$};
 
  \draw[-Latex] (t)--(c_i1);
  \draw[-Latex] (t) to (c_i-1);
  \draw[-Latex] (t) to (c_iw);
  \draw[-Latex] (t) to (c_i-w);
\end{tikzpicture}
\end{figure}

Используя обозначения и постановку задачи, введенные в пункте \label{sec:intro}, определим базу алгоритма. Целью является обучить векторное представление сообществ, используя skip-gram \cite{mikolov2013distributed} модель (см.  рис. \ref{fig-sg}) путем максимизации целевой функции над множеством $S$ сессий. В терминах NLP <<предложениями>> будут выступать сессии, а <<словами>> --- подписки пользователя.
\begin{align}
L = \sum_{s \in S}\sum_{c \in s}\sum_{-w \leq j \leq w,\, j \ne 0}\log \mathbb{P}(c_{i + j} | c_i) \label{eq1}
\end{align}
Вероятность $\mathbb{P}(c_{i + j} | c_i$ наблюдать сообщество $c_{i + j}$ в сессии рядом с заданным сообществом $c_i$ задается функцией soft-max 
\begin{align}
\mathbb{P}(c_{i + j} | c_i) = \frac{\exp(v_{c_i}^\top v_{c_{i + j}}^{'})}{\sum_{c = 1}^{|C|}\exp(v_{c_i}^\top v_{c}^{'})}\label{eq2}
\end{align}

Где ${c_k}$ и ${c_k}^{'}$ входное и выходное векторные представления сообщества $c$, параметр $w$ длина контекста сообществ, $C$ --- множество сообществ. 
Из (\ref{eq1}) и (\ref{eq2}) видно: обученная модель будет размещать сообщества в векторном пространстве так, что сообщества, встречающиеся в похожих контекстах (имеют похожие соседние сообщества в сессиях) будут иметь схожие векторы.  

\section{Модификации функции подсчета вероятности }\label{sec:prob}
Максимизация вероятности, задаваемой функцией
(\ref{eq1}), имеет существенный недостаток, а именно --- сложность вычисления ее градиента, которая обуславливается большим числом операций (пропорциональных размеру словаря, т.е. количеству сообществ $C$. Число сообществ в социальной сети исчисляется десятками миллионов).

Альтернативой soft-max является подход негативного семплирования ~\cite{mikolov2013distributed}, позволяющий существенно снизить вычислительную сложность расчета вероятности, аппроксимируя результирующую величину.

% TODO: NCE ФОРМУЛА

% Представленная функция позволяет сменить постановку задачи на научиться
% отличать целевое слово Wo от случайно выбранного слова из распределения шума
% P. Таким образом, NCE позволяет приблизительно максимизировать логарифм
% вероятности, задаваемый с помощью Softmax. Однако в данной работе гораздо
% более важным фактором выступают негативные события, в связи с чем NCE
%работает хуже, чем подход, описанный далее.

%Альтернатива подсчета полных вероятностей с помощью Softmax --- это
%негативное семплирование, предложенное в  . 

\begin{figure}[!h]
\caption{skip-gram модель для подписок}\label{fig2}
\centering
\begin{tikzpicture}
  % Dialectics
  \node[draw, fill=yellow] (t) at (6, 0) {$c_{i}$};
  \node[draw, fill=green!65] (c_i1) at (1.75, 1.5) {$c_{i + 1}$};
  \node[draw, fill=green!65] (c_K) at (3.5, 1.5) {$c_{K}$};
  \node[draw, fill=green!65] (c_i-1) at (0.25, 1.5) {$c_{i - 1}$};
  \node[draw, fill=green!65] (c_1) at (-1.5, 1.5) {$c_{1}$};
  
  \node[draw, fill=red!65] (u_1) at (5.5, 1.5) {$u_{1}$};
  \node[draw, fill=red!65] (u_M) at (7, 1.5) {$u_{M}$};
  
  \node[draw, fill=red!65] (n_1) at (9, 1.5) {$n_{1}$};
  \node[draw, fill=red!65] (n_r) at (10.5, 1.5) {$n_{r}$};
 
 
  \node at (-0.75, 1.5) {\dots};
  \node at (2.75, 1.5) {\dots};
  \node at (6.25, 1.5) {\dots};
  \node at (9.75, 1.5) {\dots};
  \node at (6, -1) {Целевое сообщество $c_i$};
  \node at (1.5, 2.5) {Подписки $S_p$};
  \node at (6, 2.5) {Отписки $S_u$};
  \node at (10, 2.5) {Случайные $S_n$};
  
  \node[draw=blue, fit=(c_1) (c_K)](f1) {};
  \node[draw=blue, fit=(u_1) (u_M)](f2) {};
  \node[draw=blue, fit=(n_1) (n_r)](f3) {};
 
  \draw[-Latex] (t) to (f1);
  \draw[-Latex] (t) to (f2);
  \draw[-Latex] (t) to (f3);

\end{tikzpicture}
\end{figure}

Техника негативного семплирования, представленная в ~\cite{airbnb} может быть адаптирована в рамках текущей задачи (см. рис. \ref{fig2}), задавая оптимизируемую функцию в виде формулы

\begin{align}
L_c = \sum_{p \in S_p} \log \frac{1}{1 + \exp(-v_p^{'}v_c)} + \sum_{u \in S_u} \log \frac{1}{1 + \exp(v_u^{'}v_c)} + \sum_{n \in S_n} \log \frac{1}{1 + \exp(v_n^{'}v_c)} \label{eq3}
\end{align}

Где $c$ --- целевое сообщество сессии $k$, $c \in s_k$, $S_p$ --- множество позитивных событий
контекста (подписки на сообщества), $S_u$ --- множество негативных событий
контекста (отписки от сообщества), $S_u = u_k$, $S_n$ --- множество случайных негативных
событий, не входящих в $S_p$ и $S_u$, число элементов множества задается гиперпараметром $r$ модели, $|S_n| = r$. Наша задача --- максимизировать данную
функцию на каждом шаге обучения, повышая, таким образом, вероятность для
позитивных событий и понижая для негативных. Оптимизация функции осуществляется стохастическим градиентным спуском. 

Экспериментально было проверено, что игнорирование отписок либо неиспользование случайных событий ведет к потере точности модели. Были опробованы упрощенные версии формулы (\ref{eq3}), полученные путем отбрасывания
поочередно последнего и предпоследнего членов выражения. Эксперимент показал, что первом случае чрезвычайно сильное
влияние оказывает начальное состояние векторов (случайное) и модель в процессе обучения не получает данных о том, что сообщества, вероятно,
не похожи друг на друга (не встречаются в одинаковых контекстах), а во втором не получает явного пользовательского отклика о негативных событиях. 

\section{Представление данных для модели}\label{sec:dataf}

Рассмотрим способы передачи данных модели.
Для каждой сессии будем составлять пакеты, состоящие из целевого
сообщества ($c$), его контекста (($S_p$) и негативных сигналов. Негативные семплы, как было
описано в \ref{sec:prob}, состоят из отписок (общие для всей сессии и, как следствие,
каждого пакета) и случайных элементов, не входящих в сессию (генерируются для
каждого пакета). ($S_u$ и $S_n$ соответственно)

Целевые сообщества будем получать, итерируясь по сессии, а способы
получения контекста будут описаны ниже. Таким образом, для каждой сессии
получим список пакетов длины сессии, которые можно передать в модель для
обучения.

Первый и самый простой подход, использующийся в классических skip-gram моделях, это представить контекст в виде соседей целевого сообщества. 
\begin{align}
S_p =\{k_{i + j} \in s, -w \leq j \leq w,\, j \ne 0\} \label{eq4}
\end{align}
Где $s$ --- сессия сообщества $c$, $i$ --- индекс сообщества $c$ в $s$. $i \leq w \leq |s| - w$
Данный
способ показывает себя довольно плохо на данных по подпискам, поскольку подписка
достаточно редкое явление и является сильным сигналом, при том часть сигналов
(отстоящих на > N сессий от целевой) будет игнорироваться.

Отметим, что средняя длина сессии в данных примерно равна 4.5 и в таком случае, можно применить альтернативные подходы, позволяющие учитывать подписки, которые были бы проигнорированы в первом случае. Кроме того, стоит принимать во внимание факт, что подписки являются долгосрочным интересом пользователя, поэтому даже далеко отстоящие по времени действия пользователя в сессии могут быть связаны между собой.

Так, можно передавать все подписки из сессии в модель, за исключением целевого сообщества.
\begin{align}
S_p =\{k_j \in s, j \ne i\} \label{eq5}
\end{align}
Однако было замечено, что в данных присутствует очень маленькое число длинных сессий (>20 элементов) крайне активных пользователей, которые никак не обрабатываются данным методом и негативно
влияют на обучение (поскольку в таком случае далеко стоящие друг от друга
события в сессии являются шумом)

Решением этой проблемы может быть нарезка больших
сессий на несколько маленьких. Однако, на практике было выяснено, что такие сессии выступают аномалией и их стоит игнорировать. 

Отдельно стоит отметить, что при таком подходе оптимизируемая функция $L_c$ (\ref{eq3}) зависит от длины сессии, поэтому ее следует нормализовать. Таким образом, конечная функция, оптимизируемая моделью, будет выглядеть следующим образом: 

\begin{align}
F_c = \frac{L_c}{|S_p| + |S_u| + |S_b|} \label{eq6}
\end{align}

\section{Модификация модели для работы с данными по <<лайкам>>}\label{sec:datal}

Изменим модель для работы с данными по <<лайкам>>. Возьмем базовую модель, описанную в предыдущих пунктах. Отметим, что мы больше не имеем явных негативных пользовательских сигналов, поэтому формула (\ref{eq3}) будет переписана в виде:
\begin{align}
L_c = \sum_{p \in S_p} \log \frac{1}{1 + \exp(-v_p^{'}v_c)} + \sum_{n \in S_n} \log \frac{1}{1 + \exp(v_n^{'}v_c)} \label{eq7}
 \end{align}
Где $c$ --- целевое сообщество, $S_p$ --- список позитивных событий
контекста (лайки записей сообществ), $S_n$ --- множество случайных негативных событий, не входящих в $S_p$. 

Несмотря на то, что $S_p$ для предоставленного набора данных можно представлять в виде (\ref{eq5}), более гибким способом будет использование формулы (\ref{eq4}), поскольку лайки являются гораздо более частым событием и отвечают за краткосрочный интерес пользователя, что ведет к несвязности далеких по времени событий. Кроме того, для одинакового временного промежутка количество лайков будет существенно больше числа подписок и представление в виде (\ref{eq5}) не позволит работать модели по обоим типам данных за одинаковый промежуток времени. 

Конечную функцию оптимизации  $L_c$ можно не нормализовать, поскольку $|S_p|$ и $|S_u|$ фиксированы.

Рассмотрим теперь еще одну версию модели, которая способна обрабатывать оба типа событий: как подписки, так и лайки. Простым способом будет варьировать оптимизируемую функцию в зависимости от типа контекста. Так, для подписок будем использовать (\ref{eq6}), а для лайков (\ref{eq7}) --- уже нормализованную. Методы получения необходимых данных на шаге обучения оставим такими же. Стоит учесть, что подписка является гораздо более важным сигналом, поэтому предлагается усилить влияние таких событий умножением на задаваемый коэффициент, который следует подбирать в зависимости от соотношения лайков к подпискам в исходных данных. В итоге получаем следующие функции оптимизации на шаге обучения.

Для подписок:
\begin{align*}
G_c = qF_c
\end{align*}

Где $q$ является дополнительным гиперпараметром модели. В проводимых экспериментах $q$ был равен 3.

Для лайков:
\begin{align*}
G_c = \frac{L_c}{|S_p| +|S_b|} 
\end{align*}
Где $L_c$ задается формулой  (\ref{eq7})

\chapterconclusion

В этой главе был разработан алгоритм векторного представления сообществ. Были рассмотрены 3 версии алгоритма: для работы с сессионными данными пользователей по подпискам, по лайкам и смешанным.

Итоговая модель обучается следующим образом: получив на вход пакет из
целевого сообщества, списка позитивных и негативных сигналов, посчитаем
значение оптимизуемой функции. Используем стохастический градиентный спуск для обновления весов входной и выходной матриц, заканчивая шаг обучения. Генерируемые пакеты и оптимизируемая функция зависят от типа данных. (\ref{sec:dataf} и \ref{sec:datal})

Интересующий нас результат находится в первой (входной) матрице, которая
представляет из себя векторы сообществ $c_k$, т. е. в строке $k$ содержится векторное
представления сообщества $c_k$. Дополнительная (выходная) матрица
интерпретируется как векторное представление контекстов.

Мы обучили алгоритм, результатом которого является матрица векторного
представления сообществ. Предлагаемый алгоритм имеет ряд довольно весомых
преимуществ по сравнению с альтернативными подходами. 

Полученные векторы
имеют малую размерность (которую можно задать в качестве параметра), что
делает применение векторов в качестве входных данных какой-либо другой
модели более удобным. 

При построении векторов учитывается сессионная и
коллаборативная информация, таким образом, полученный результат будет
учитывать как сходство сообществ в похожих контекстах, так и давность данных
(предпочтения пользователя могут меняться со временем). 

Кроме того, входные данные для модели не требуют дополнительной ручной разметки. Все, что нужно модели для работы, --- пользовательские действия, которые могут быть никак не обработаны. Таким образом, использование модели в реальной системе является крайне простым.

\chapter{РЕЗУЛЬТАТЫ ЭКСПЕРИМЕНТОВ}

В качестве дополнительной задачи позволяющей понять, что полученные
векторы действительно несут полезную информацию (правильным образом
представляют положение сообществ в пространстве) я решил несколько подзадач
описанных в \ref{sec:class} --- \ref{sec:next-action} и сравнил полученные результаты с альтернативными
алгоритмами, описанными в первой главе.

\section{Кластеризация полученных векторов}

Векторы, полученные из модели, описанной в главе 2, можно представить в
более удобном формате для анализа. Например, их можно кластеризировать и
рассмотреть несколько кластеров. В качестве алгоритма кластеризации был
использован k-means, результатом работы которого является разбиение итоговых
векторов на 256 кластеров. Для визуализации был применен алгоритм t-SNE,
позволяющий понизить размерность векторов до 2, для последующего
представления с помощью библиотеки matplotplib. (См. рисунок 1)

TODO: рисунок 1

Рассмотрим более подробно группы кластеров, отмеченными на рис. 1
цифрами 1, 2 и 3.

Группа 1 имеет выраженную особенность в виде удаленности от остальных
точек. Действительно, при более близком рассмотрении сообществ первой группы,
становится ясно, что они сильно специфичны: это сообщества продавцов и
покупателей ТК «Садовод». Значит, модель умеет отличать сообщества узко
направленной тематики и помещать их рядом в конечном векторном пространстве.

Группа 2 является скоплением кластеров сообществ пользователей
Казахстана, ведущихся на казахском языке. Стоит отметить, что данная группа
также отстоит от прочих сообществ и, кроме того, является достаточно большой
(данная группа содержит более 10 различных кластеров). Таким образом,
полученная модель умеет выделять сообщества пользователей, относящихся к
меньшинству пользователей платформы. (Пользователей, говорящих на казахском
языке, значительно меньше русскоговорящих)

Группа 3 содержит в себе два скопления кластеров, находящихся довольно
близко друг к другу и представляет из себя поклонников восточной культуры.
Представленные кластеры в первом скоплении являются сообществами о Корее и
корейской популярной музыки, во втором – сообщества о Японии и японской
анимации. Так, модель умеет не только группировать сообщества близкие по
тематике (Корея и корейская музыка), но и схожие по типажу (азиатская культура).

TODO: переписать все что сверху

Отдельно приведем пример кластера, связанного с изучением английского
языка в таблице 1. Полная ссылка, указанная в таблице, имеет вид
(vk.com/[значение])

TODO: таблица

\section{Обучение алгоритма классификации сообществ}\label{sec:class}

Сообщества <<Вконтакте>> имеют категории, задаваемые администраторами
(например, спорт, СМИ и т. д.). Мы можем обучить алгоритм классификации,
используя векторы, полученные в ходе работы алгоритмов, в качестве входных
данных модели. Так же посмотрим на результаты обучения модели на необработанных данных. (В таком случае сообщество представляем в виде вектора: 1 в этом векторе стоят на позициях, равных номеру сессии, в котором встретилось сообщество. Отписки будем игнорировать.)  
Будем обучать модель на дополнительных размеченных данных,
которые содержат пары из идентификатора сообщества и его категории.

В качестве алгоритма классификации я использовал несколько алгоритмов (Gradient boosting classifier, Ada boost classifier, linear support vector classifier), для каждой модели был выбран лучший вариант. Усредненные результаты работы классификаторов, обученных на разных векторах, указаны в таблицах: в ячейках указаны значения F1 меры посчитанной для заданного класса. Последняя строчка содержит среднее значение F1 меры по всем классам, учитывая частоту, с которой они встречаются в наборе данных. Последняя строка содержит численное значение этой информации.

В таблицах \ref{tab-subs-g}, \ref{tab-subs-d} указаны результаты обучения классификатора по векторам, обученных на данных по сообществам. В таблицах \ref{tab-likes-g}, \ref{tab-likes-d} указаны результаты по векторам, обученных на данных по лайкам. В таблицах \ref{tab-combined-g}, \ref{tab-combined-d} указаны результаты по векторам, обученных на комбинированных данных. Для ALS и LDA обучение строится таким же путем, как было указано в пунктах \ref{sec:als} \ref{sec:lda}. Обучение производилось для 5 самых часто встречающихся категорий.

Важно отметить, что реализации алгоритмов ALS и LDA пакета spark крайне требовательны к характеристикам оборудования, на котором они обучаются. В связи с этим, результаты, представленные в таблицах \ref{tab-subs-g} - \ref{tab-combined-d} были получены путем обучения моделей на меньшем объеме данных, а именно: 45000 сессий с 1200 различными группами, каждая из которых встречается как минимум 50 раз во всех сессиях в данных по сообществам и 87000 сессий с 2700 различными группами в данных по лайкам.

\begin{table}[!h]
\caption{Классификация общих категорий (данные по сообществам)}\label{tab-subs-g}
\centering
\begin{tabular}{|*{18}{c|}}\hline
    Категория & \thead{Предложенный \\ алгоритм}  & Raw data & ALS & LDA & Freq \\\hline
Ремонт и строительство     & 0.692 & 0.154 & 0.133 & \textbf{0.706} & 50 \\\hline
Рецепты и еда                       & 0.558 & 0.194 & 0.074 & \textbf{0.578} & 88 \\\hline
Здоровье и красота             & \textbf{0.444} & 0.177 & 0.062 & 0.367 & 114 \\\hline
Культурное общество         & \textbf{0.255} & 0.159 & 0.182 & 0.091 & 127  \\\hline
Развлечения                           & 0.741 & 0.547 & 0.674 & \textbf{0.762} & 372 \\\hline
Средн. взвешн.                      & \textbf{0.584} & 0.362 & 0.401 & 0.572 & --- \\\hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Классификация подробных категорий (данные по сообществам)}\label{tab-subs-d}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Категория & Предложенный алгоритм  & Raw data & ALS & LDA & Freq \\\hline
Кино                    & \textbf{0.593} & 0.129 & 0.235 & 0.375 & 44  \\\hline
Литература       & \textbf{0.400} & 0.240 & 0.111 & 0.273 & 46 \\\hline
Уход за собой   & \textbf{0.593} & 0.067 & 0.250 & 0.364 & 54 \\\hline
Рецепты и еда  & 0.722 & 0.167 & 0.293 & \textbf{0.750} & 88 \\\hline
Юмор                  & \textbf{0.814} & 0.500 & 0.644 & 0.768 & 241 \\\hline
Средн. взвеш.   & \textbf{0.703} & 0.325 & 0.452 & 0.637  & --- \\\hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Классификация общих категорий (данные по лайкам)}\label{tab-likes-g}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Категория & Предложенный алгоритм  & Raw data & ALS & LDA & Freq \\\hline
46988013                          & 0.160 & 0.182 & 0.250 & 0.133 & 80 \\\hline
7394d6a2                          & 0.375 & 0.123 & 0.059 & 0.293 & 110 \\\hline
9d802769                          & 0.686 & 0.227 & 0.146 & 0.698 & 115 \\\hline
Здоровье и красота       & 0.364 & 0.140 & 0.051 & 0.171 & 117  \\\hline
Культурное общество   & 0.694 & 0.502 & 0.649 & 0.684 & 390 \\\hline
Средн. взвеш.                  & 0.574 & 0.307 & 0.377 & 0.518 & --- \\\hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Классификация подробных категорий (данные по лайкам)}\label{tab-likes-d}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Категория & Предложенный алгоритм  & Raw data & ALS & LDA & Freq \\\hline
ef14d17c             & 0.578 & 0.125 & 0.261 & 0.533 & 80 \\\hline
14c24f5a             & 0.488 & 0.065 & 0.077 & 0.564 & 81 \\\hline
Кино            & 0.457 & 0.154 & 0.163 & 0.400 & 92 \\\hline
a4e3f544             & 0.400 & 0.065 & 0.321 & 0.431 & 114  \\\hline
Литература            & 0.637 & 0.327 & 0.333 & 0.658 & 157 \\\hline
Средн. взвешн.  & 0.529 & 0.167 & 0.236 & 0.522 & --- \\\hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Классификация общих категорий (комбинированные данные)}\label{tab-combined-g}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Категория & Предложенный алгоритм  & Raw data & ALS & LDA & Freq \\\hline
Ремонт и строительство            & NA & NA & NA & NA & 50 \\\hline
Рецепты и еда            & NA & NA & NA & NA & 88 \\\hline
Здоровье и красота             & NA & NA & NA &  & 114 \\\hline
Культурное общество            & NA & NA & NA &NA   & 127  \\\hline
Развлечения              & NA & NA & NA & NA & 372 \\\hline
Средн. взвеш.  & NA & NA & NA &  & --- \\\hline
\end{tabular}
\end{table}

\begin{table}[!h]
\caption{Классификация подробных категорий (комбинированные данные)}\label{tab-combined-d}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Категория & Предложенный алгоритм  & Raw data & ALS & LDA & Freq \\\hline
Ремонт и строительство            & NA & NA & NA & NA & 50 \\\hline
Рецепты и еда            & NA & NA & NA & NA & 88 \\\hline
Здоровье и красота             & NA & NA & NA &  & 114 \\\hline
Культурное общество            & NA & NA & NA &NA   & 127  \\\hline
Развлечения              & NA & NA & NA & NA & 372 \\\hline
Средн. взвеш.  & NA & NA & NA &  & --- \\\hline
\end{tabular}
\end{table}


\section{Предсказание следующего действия}\label{sec:next-action}
Пользуясь полученными векторами можно попытаться предсказать следующее действие пользователя по его истории. Стоит отметить, что задача, поставленная таким способом крайне сложная и требует дополнительной информации. Облегчим постановку задачи: предсказать топ-50 кандидатов следующего действия пользователя. Для такой задачи можем посчитать процент попадания настоящего действия в топ. В таблице указаны значения этой величины, посчитанные на векторах по разным наборам данных. Сложим вектора сообществ из истории и найдем ближайшие вектора к полученному, используя метрику векторного сходства (cosin similarity). Отранжируем их по убыванию, и возьмем 50 кандидатов с наибольшим значением метрики.

\begin{table}[!h]
\caption{Предсказание следующего действия} \label{tab2-next-action}
\centering
\begin{tabular}{|*{18}{c|}}\hline
Алгоритм & Подписки & Лайки & Комбинированные \\\hline
ALS                        & 0.035 & 0.066  & NA \\\hline
LDA                       & 0.095 & 0.121  & NA\\\hline
Предложенный & 0.405 & 0.287 & NA \\\hline
\end{tabular}
\end{table}

\section{Дополнительные подзадачи}

TODO

%% Макрос для заключения. Совместим со старым стилевиком.
\startconclusionpage

TODO

\printmainbibliography

%% После этой команды chapter будет генерировать приложения, нумерованные русскими буквами.
%% \startappendices из старого стилевика будет делать то же самое
\appendix

\end{document}
